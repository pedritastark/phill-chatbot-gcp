#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Capa de decisiÃ³n LLM para Phill Chatbot.
Determina si usar procesos automatizados o conversaciÃ³n con asesor financiero.
"""

import os
import json
import google.generativeai as genai
from typing import Dict, Tuple, Optional
from datetime import datetime

class LLMDecisionLayer:
    """Capa de decisiÃ³n que usa Google AI Studio (Gemini) para determinar el flujo del chatbot."""
    
    def __init__(self):
        # Configurar Google AI Studio
        api_key = os.environ.get('GOOGLE_AI_API_KEY')
        if not api_key:
            raise ValueError("GOOGLE_AI_API_KEY no estÃ¡ configurada en las variables de entorno")
        
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel('gemini-pro')
        
        # ConfiguraciÃ³n del sistema
        self.system_prompt = """
Eres un asistente financiero inteligente que decide cÃ³mo responder a los usuarios.

Tu tarea es analizar el mensaje del usuario y determinar si debe ser procesado por:
1. PROCESO_AUTOMATIZADO: Para tareas especÃ­ficas como registrar gastos, ingresos, consultar balance, etc.
2. CONVERSACION_ASESOR: Para consultas complejas, consejos personalizados, o conversaciones generales

PROCESOS AUTOMATIZADOS (usa PROCESO_AUTOMATIZADO):
- Registrar gastos especÃ­ficos: "GastÃ© $50,000 en comida"
- Registrar ingresos: "RecibÃ­ $1,000,000 de salario"
- Consultar balance: "Â¿CuÃ¡nto tengo?"
- Ver resumen: "MuÃ©strame mis gastos del mes"
- Crear recordatorios: "RecuÃ©rdame pagar el 15"
- Solicitar tips bÃ¡sicos: "Dame un consejo financiero"

CONVERSACIÃ“N CON ASESOR (usa CONVERSACION_ASESOR):
- Consultas complejas: "Â¿CÃ³mo puedo mejorar mis finanzas?"
- AnÃ¡lisis personalizado: "Â¿QuÃ© me recomiendas para ahorrar?"
- Conversaciones generales: "Hola, Â¿cÃ³mo estÃ¡s?"
- Preguntas sobre inversiones: "Â¿DÃ³nde deberÃ­a invertir mi dinero?"
- PlanificaciÃ³n financiera: "Â¿CÃ³mo hago un presupuesto?"
- Dudas conceptuales: "Â¿QuÃ© es el interÃ©s compuesto?"

Responde SOLO con un JSON que contenga:
{
    "decision": "PROCESO_AUTOMATIZADO" o "CONVERSACION_ASESOR",
    "confidence": 0.0-1.0,
    "reasoning": "explicaciÃ³n breve de por quÃ© tomaste esta decisiÃ³n",
    "intent_suggestion": "intent especÃ­fico si es PROCESO_AUTOMATIZADO, null si es CONVERSACION_ASESOR"
}
"""

    def analyze_user_intent(self, message: str, user_context: Optional[Dict] = None) -> Dict:
        """
        Analiza la intenciÃ³n del usuario y decide el flujo apropiado.
        
        Args:
            message: Mensaje del usuario
            user_context: Contexto adicional del usuario (historial, preferencias, etc.)
            
        Returns:
            Diccionario con la decisiÃ³n y metadatos
        """
        try:
            # Construir el prompt con contexto
            context_info = ""
            if user_context:
                context_info = f"\nContexto del usuario: {json.dumps(user_context, ensure_ascii=False)}"
            
            full_prompt = f"""
{self.system_prompt}

Mensaje del usuario: "{message}"
{context_info}

Analiza este mensaje y decide el flujo apropiado.
"""
            
            # Generar respuesta con Gemini
            response = self.model.generate_content(full_prompt)
            
            # Parsear la respuesta JSON
            try:
                decision_data = json.loads(response.text.strip())
                
                # Validar estructura
                required_fields = ['decision', 'confidence', 'reasoning']
                if not all(field in decision_data for field in required_fields):
                    raise ValueError("Respuesta JSON incompleta")
                
                # Validar valores
                if decision_data['decision'] not in ['PROCESO_AUTOMATIZADO', 'CONVERSACION_ASESOR']:
                    raise ValueError("DecisiÃ³n invÃ¡lida")
                
                if not 0.0 <= decision_data['confidence'] <= 1.0:
                    raise ValueError("Confianza fuera de rango")
                
                return decision_data
                
            except (json.JSONDecodeError, ValueError) as e:
                print(f"Error parseando respuesta LLM: {e}")
                print(f"Respuesta cruda: {response.text}")
                
                # Fallback: anÃ¡lisis simple basado en palabras clave
                return self._fallback_analysis(message)
                
        except Exception as e:
            print(f"Error en anÃ¡lisis LLM: {e}")
            return self._fallback_analysis(message)
    
    def _fallback_analysis(self, message: str) -> Dict:
        """AnÃ¡lisis de fallback cuando el LLM falla."""
        message_lower = message.lower()
        
        # Palabras clave para procesos automatizados
        automated_keywords = [
            'gastÃ©', 'gastar', 'comprÃ©', 'paguÃ©', 'registrar', 'anotar',
            'recibÃ­', 'ingresÃ³', 'salario', 'balance', 'resumen', 'cuÃ¡nto',
            'recordar', 'recordatorio', 'avisar', 'tip', 'consejo'
        ]
        
        # Palabras clave para conversaciÃ³n con asesor
        advisor_keywords = [
            'cÃ³mo', 'quÃ©', 'por quÃ©', 'ayuda', 'recomienda', 'mejorar',
            'invertir', 'ahorrar', 'presupuesto', 'planificar', 'finanzas'
        ]
        
        automated_score = sum(1 for keyword in automated_keywords if keyword in message_lower)
        advisor_score = sum(1 for keyword in advisor_keywords if keyword in message_lower)
        
        if automated_score > advisor_score:
            return {
                "decision": "PROCESO_AUTOMATIZADO",
                "confidence": 0.7,
                "reasoning": "AnÃ¡lisis de fallback: detectadas palabras clave de proceso automatizado",
                "intent_suggestion": None
            }
        else:
            return {
                "decision": "CONVERSACION_ASESOR",
                "confidence": 0.6,
                "reasoning": "AnÃ¡lisis de fallback: detectadas palabras clave de conversaciÃ³n",
                "intent_suggestion": None
            }
    
    def get_advisor_response(self, message: str, user_context: Optional[Dict] = None) -> str:
        """
        Genera una respuesta como asesor financiero usando el LLM.
        
        Args:
            message: Mensaje del usuario
            user_context: Contexto del usuario (historial financiero, etc.)
            
        Returns:
            Respuesta del asesor financiero
        """
        try:
            # Construir contexto del asesor
            advisor_prompt = f"""
Eres Phill, un asesor financiero personal experto y amigable. Tu personalidad es:
- Cercano y colombiano (usa expresiones como "parcero", "Â¡Esoooo!", "Â¡Dale!")
- Conocedor de finanzas personales
- PrÃ¡ctico y directo en tus consejos
- Motivador y positivo
- Usas emojis apropiadamente

Contexto del usuario: {json.dumps(user_context, ensure_ascii=False) if user_context else "Sin contexto previo"}

Mensaje del usuario: "{message}"

Responde como Phill, el asesor financiero. SÃ© Ãºtil, prÃ¡ctico y mantÃ©n el tono amigable colombiano.
Si el usuario pregunta algo muy especÃ­fico sobre sus datos, puedes sugerirle usar los comandos automatizados.
"""
            
            response = self.model.generate_content(advisor_prompt)
            return response.text.strip()
            
        except Exception as e:
            print(f"Error generando respuesta de asesor: {e}")
            return "Â¡Hola parcero! Soy Phill, tu asesor financiero. Â¿En quÃ© te puedo ayudar hoy? ðŸ’°"

# Instancia global
llm_decision_layer = LLMDecisionLayer()
